{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigEarthNet Base Functions\n",
    "> A collection of common function that are applied to BEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import functools\n",
    "import urllib\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from numbers import Real\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Set, Tuple, Union, Sequence\n",
    "\n",
    "import appdirs\n",
    "import dateutil\n",
    "import fastcore.all as fc\n",
    "import geopandas\n",
    "import pandas as pd\n",
    "from fastcore.basics import compose\n",
    "from fastcore.dispatch import typedispatch\n",
    "from pydantic import AnyHttpUrl, validate_arguments\n",
    "from shapely.geometry import LineString, Point, Polygon, box\n",
    "\n",
    "import bigearthnet_common.constants as ben_constants\n",
    "from bigearthnet_common.constants import OLD2NEW_LABELS_DICT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "USER_DIR = Path(appdirs.user_data_dir(\"bigearthnet\"))\n",
    "USER_DIR.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from datetime import date\n",
    "\n",
    "from dateutil.parser import ParserError\n",
    "from fastcore.test import ExceptionExpected, test_close, test_eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def _download_and_cache_url(url: AnyHttpUrl, force_download: bool = False):\n",
    "    \"\"\"\n",
    "    Simply download contents of url to the default user directory.\n",
    "    Allow to redownload with `force_download`\n",
    "    \"\"\"\n",
    "    fp = USER_DIR / Path(url).name\n",
    "    if not fp.exists() or force_download:\n",
    "        response = urllib.request.urlopen(url).read()\n",
    "        fp.write_bytes(response)\n",
    "    return fp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BEN-JSON helper functions\n",
    "\n",
    "A couple of safe parsing functions that guarantee flexible reading of the default BigEarthNet json entries.\n",
    "\n",
    "Usually, you should not use any of these parsing functions directly, but use one of the higher-level functions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_datetime(inp: Union[str, datetime]) -> datetime:\n",
    "    \"\"\"\n",
    "    Parses an input into a `datetime` object.\n",
    "    Will try its best to infer the correct format from a string.\n",
    "    If a `datetime` object is already provided it will be returned.\n",
    "    Otherwise it will raise an error.\n",
    "    \"\"\"\n",
    "    return _parse_datetime(inp)\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def _parse_datetime(acquisition_date: str) -> datetime:\n",
    "    return compose(dateutil.parser.parse, _parse_datetime)(acquisition_date)\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def _parse_datetime(acquisition_date: datetime) -> datetime:\n",
    "    return acquisition_date\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def _parse_datetime(acquisition_date: object) -> None:\n",
    "    raise TypeError(\"Could not parse acquisition_date!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = parse_datetime(\"2017-06-13 10:10:31\")\n",
    "d2 = parse_datetime(\"13.06.2017 10:10:31\")\n",
    "d3 = parse_datetime(datetime(year=2017, month=6, day=13, hour=10, minute=10, second=31))\n",
    "\n",
    "test_eq(d1, d2)\n",
    "test_eq(d2, d3)\n",
    "\n",
    "with ExceptionExpected(ex=ParserError, regex=\"format\"):\n",
    "    parse_datetime(\"large_tile\")\n",
    "\n",
    "with ExceptionExpected(ex=TypeError, regex=\"parse\"):\n",
    "    parse_datetime(42)\n",
    "\n",
    "with ExceptionExpected(ex=TypeError, regex=\"parse\"):\n",
    "    parse_datetime(date(year=2017, month=10, day=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _get_box_from_two_coords(p1: Tuple[Real, Real], p2: Tuple[Real, Real]) -> Polygon:\n",
    "    \"\"\"\n",
    "    Get the polygon that bounds the two coordinates.\n",
    "    These values should be supplied as numerical values.\n",
    "    \"\"\"\n",
    "    get_bounds = lambda geom: geom.bounds\n",
    "    box_from_bounds = lambda bounds: box(*bounds)\n",
    "    return compose(LineString, get_bounds, box_from_bounds)([p1, p2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "box1 = _get_box_from_two_coords([0, 0], [2, 2])\n",
    "box2 = _get_box_from_two_coords([2, 2], [0, 0])\n",
    "box3 = _get_box_from_two_coords([0, 2], [2, 0])\n",
    "test_eq(box1, box2)\n",
    "test_eq(box1, box3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def box_from_ul_lr_coords(ulx: Real, uly: Real, lrx: Real, lry: Real) -> Polygon:\n",
    "    \"\"\"\n",
    "    Build a box (`Polygon`) from upper left x/y and lower right x/y coordinates.\n",
    "\n",
    "    This specification is the default BigEarthNet style.\n",
    "    \"\"\"\n",
    "    return _get_box_from_two_coords([ulx, uly], [lrx, lry])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = box_from_ul_lr_coords(ulx=0, uly=4, lrx=4, lry=0)\n",
    "b2 = Polygon([[0, 0], [0, 4], [4, 4], [4, 0], [0, 0]])\n",
    "assert isinstance(b1, Polygon)\n",
    "assert b1.equals(b2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# example of how to use geopandas to read in weirdly shaped data\n",
    "import geopandas\n",
    "from shapely.geometry import box, Point\n",
    "\n",
    "# CRS with easting/northing input, i.e. no input axis-swap required\n",
    "wkt_crs = 'PROJCS[\"WGS 84 / UTM zone 34N\",GEOGCS[\"WGS 84\",DATUM[\"WGS_1984\",SPHEROID[\"WGS 84\",6378137,298.257223563,AUTHORITY[\"EPSG\",\"7030\"]],AUTHORITY[\"EPSG\",\"6326\"]],PRIMEM[\"Greenwich\",0,AUTHORITY[\"EPSG\",\"8901\"]],UNIT[\"degree\",0.0174532925199433,AUTHORITY[\"EPSG\",\"9122\"]],AXIS[\"Latitude\",NORTH],AXIS[\"Longitude\",EAST],AUTHORITY[\"EPSG\",\"4326\"]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"latitude_of_origin\",0],PARAMETER[\"central_meridian\",21],PARAMETER[\"scale_factor\",0.9996],PARAMETER[\"false_easting\",500000],PARAMETER[\"false_northing\",0],UNIT[\"metre\",1,AUTHORITY[\"EPSG\",\"9001\"]],AXIS[\"Easting\",EAST],AXIS[\"Northing\",NORTH],AUTHORITY[\"EPSG\",\"32634\"]]'\n",
    "x1, y1, x2, y2 = 499980, 7046040, 501180, 7044840\n",
    "shapes = [_get_box_from_two_coords([x1, y1], [x2, y2])]\n",
    "\n",
    "s = geopandas.GeoSeries(shapes, crs=wkt_crs)\n",
    "assert s.is_valid.all()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# crs with northing/easting input, i.e. input axis-swap required\n",
    "north_east_crs = \"epsg:2953\"\n",
    "enc_point = Point(1099489.55, 9665176.75)\n",
    "tfm_points = geopandas.GeoSeries([enc_point], crs=north_east_crs).to_crs(epsg=\"4326\")\n",
    "long, lat = tfm_points.x[0], tfm_points.y[0]\n",
    "\n",
    "# _golden values_ from http://epsg.io/\n",
    "# http://epsg.io/transform#s_srs=2953&t_srs=4326&x=1099489.55&y=9665176.75\n",
    "ref_long, ref_lat = (-94.375, 63.25)\n",
    "test_close([long, lat], [ref_long, ref_lat], eps=0.1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common BEN patch checks and transformations\n",
    "\n",
    "These tiny functions mainly allow the user to write cleaner code by importing them instead of writing lambda functions.\n",
    "All functions use caches to guarantee fast lookups, so feel free to use them on large data.\n",
    "\n",
    "The most relevant functions are:\n",
    "- check if patch name in cloud/snow collection\n",
    "    - `is_snowy_patch`\n",
    "    - `is_cloudy_shadowy_patch`\n",
    "- Retrieve the original split by looking up the patch name\n",
    "    - `get_original_split_from_patch_name`\n",
    "- Convert the old 43-label nomenclature to the new 19-label variant\n",
    "    - `old2new_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "# PATCHES_WITH_SNOW_URL = \"http://bigearth.net/static/documents/patches_with_seasonal_snow.csv\"\n",
    "# PATCHES_WITH_CLOUD_AND_SHADOW_URL = \"http://bigearth.net/static/documents/get_patches_with_cloud_and_shadow.csv\"\n",
    "PATCHES_WITH_SNOW_URL = \"https://git.tu-berlin.de/k.clasen/ben-mirror/-/raw/master/patches_with_seasonal_snow.csv\"\n",
    "PATCHES_WITH_CLOUD_AND_SHADOW_URL = \"https://git.tu-berlin.de/k.clasen/ben-mirror/-/raw/master/patches_with_cloud_and_shadow.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def _conv_single_col_csv_to_set(\n",
    "    url: AnyHttpUrl, name: str = \"Name\", force_download: bool = False\n",
    ") -> Set:\n",
    "    \"\"\"\n",
    "    Given a url to a CSV file *without* a header\n",
    "    line and only a single column, return the set of\n",
    "    all values.\n",
    "\n",
    "    Will write remote csv to disk for better performance.\n",
    "    Set `force_download` to re-download the file.\n",
    "    \"\"\"\n",
    "    fp = _download_and_cache_url(url, force_download=force_download)\n",
    "    series = pd.read_csv(\n",
    "        fp,\n",
    "        names=[name],\n",
    "        squeeze=True,\n",
    "    )\n",
    "    return set(series.values)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_patches_with_seasonal_snow(force_download: bool = False) -> Set:\n",
    "    \"\"\"List all patches with seasonal snow from **original** BigEarthNet dataset.\"\"\"\n",
    "    return _conv_single_col_csv_to_set(\n",
    "        PATCHES_WITH_SNOW_URL, force_download=force_download\n",
    "    )\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_patches_with_cloud_and_shadow(force_download: bool = False) -> Set:\n",
    "    \"\"\"List all patches with cloud and shadow from **original** BigEarthNet dataset.\"\"\"\n",
    "    return _conv_single_col_csv_to_set(\n",
    "        PATCHES_WITH_CLOUD_AND_SHADOW_URL, force_download=force_download\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_patches = get_patches_with_seasonal_snow()\n",
    "assert len(snow_patches) == ben_constants.BEN_SNOWY_PATCHES_COUNT\n",
    "\n",
    "cloud_and_shadow_patches = get_patches_with_cloud_and_shadow()\n",
    "assert len(cloud_and_shadow_patches) == ben_constants.BEN_CLOUDY_OR_SHADOWY_PATCHES_COUNT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def is_snowy_patch(patch_name: str):\n",
    "    \"\"\"\n",
    "    Fast function that checks whether `patch_name` is a patch\n",
    "    that contains a lot of seasonal snow.\n",
    "    \"\"\"\n",
    "    return patch_name in get_patches_with_seasonal_snow()\n",
    "\n",
    "\n",
    "@validate_arguments\n",
    "def is_cloudy_shadowy_patch(patch_name: str):\n",
    "    \"\"\"\n",
    "    Fast function that checks whether `patch_name` is a patch\n",
    "    that contains a lot of shadow or is obstructed by clouds.\n",
    "    \"\"\"\n",
    "    return patch_name in get_patches_with_cloud_and_shadow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# doesn't check if name is sensible\n",
    "assert is_snowy_patch(\"hello\") == False\n",
    "assert is_cloudy_shadowy_patch(\"hello\") == False\n",
    "\n",
    "assert is_snowy_patch(\"S2A_MSIL2A_20180205T100211_2_0\") == True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@functools.lru_cache()\n",
    "@validate_arguments\n",
    "def patches_from_original_train_split(\n",
    "    split_url: AnyHttpUrl = \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_19-classes_models/-/raw/master/splits/train.csv\",\n",
    "    force_download: bool = False,\n",
    ") -> Set:\n",
    "    \"\"\"\n",
    "    List all train patches from the original train/validation/test split.\n",
    "    There are two possible sources:\n",
    "\n",
    "    1. https://git.tu-berlin.de/rsim/BigEarthNet-S2_19-classes_models/-/raw/master/splits/train.csv\n",
    "    2. https://git.tu-berlin.de/rsim/BigEarthNet-S2_43-classes_models/-/raw/master/splits/train.csv\n",
    "\n",
    "    While writing this function, there is **no** difference between these two splits.\n",
    "    But this may change in the future!\n",
    "    \"\"\"\n",
    "    return _conv_single_col_csv_to_set(split_url, force_download=force_download)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "@validate_arguments\n",
    "def patches_from_original_validation_split(\n",
    "    split_url: AnyHttpUrl = \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_19-classes_models/-/raw/master/splits/val.csv\",\n",
    "    force_download: bool = False,\n",
    ") -> Set:\n",
    "    \"\"\"\n",
    "    List all validation patches from the original train/validation/test split.\n",
    "    There are two possible sources:\n",
    "\n",
    "    1. https://git.tu-berlin.de/rsim/BigEarthNet-S2_19-classes_models/-/raw/master/splits/train.csv\"\n",
    "    2. \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_43-classes_models/-/raw/master/splits/train.csv\"\n",
    "\n",
    "    While writing this function, there is **no** difference between these two splits.\n",
    "    But this may change in the future!\n",
    "    \"\"\"\n",
    "    return _conv_single_col_csv_to_set(split_url, force_download=force_download)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "@validate_arguments\n",
    "def patches_from_original_test_split(\n",
    "    split_url: AnyHttpUrl = \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_19-classes_models/-/raw/master/splits/test.csv\",\n",
    "    force_download: bool = False,\n",
    ") -> Set:\n",
    "    \"\"\"\n",
    "    List all test patches from the original train/validation/test split.\n",
    "    There are two possible sources:\n",
    "\n",
    "    1. https://git.tu-berlin.de/rsim/BigEarthNet-S2_19-classes_models/-/raw/master/splits/test.csv\n",
    "    2. https://git.tu-berlin.de/rsim/BigEarthNet-S2_43-classes_models/-/raw/master/splits/test.csv\n",
    "\n",
    "    While writing this function, there is **no** difference between these two splits.\n",
    "    But this may change in the future!\n",
    "    \"\"\"\n",
    "    return _conv_single_col_csv_to_set(split_url, force_download=force_download)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "train1 = patches_from_original_train_split()\n",
    "train2 = patches_from_original_train_split(\n",
    "    \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_43-classes_models/-/raw/master/splits/train.csv\"\n",
    ")\n",
    "assert len(train1 - train2) == 0\n",
    "\n",
    "val1 = patches_from_original_validation_split()\n",
    "val2 = patches_from_original_validation_split(\n",
    "    \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_43-classes_models/-/raw/master/splits/val.csv\"\n",
    ")\n",
    "assert len(val1 - val2) == 0\n",
    "\n",
    "\n",
    "test1 = patches_from_original_test_split()\n",
    "test2 = patches_from_original_test_split(\n",
    "    \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_43-classes_models/-/raw/master/splits/test.csv\"\n",
    ")\n",
    "assert len(test1 - test2) == 0\n",
    "\n",
    "assert len(test1) < len(train1)\n",
    "assert len(val1) < len(train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def get_original_split_from_patch_name(patch: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Returns \"train\"/\"validation\"/\"test\" or `None`.\n",
    "    The value is retrieved from the original BigEarthNet\n",
    "    train/validation/test split. If the input is not present\n",
    "    in any split, it will return `None` and raise a UserWarning.\n",
    "    This happens for patches that are either in the\n",
    "    cloud/shadow or seasonal snow set or if there exists no 19-label target.\n",
    "\n",
    "    The splits are from the 19-classes version.\n",
    "    While writing this function there was no difference between the\n",
    "    19-classes and the 43-classes version.\n",
    "    \"\"\"\n",
    "    train = patches_from_original_train_split()\n",
    "    validation = patches_from_original_validation_split()\n",
    "    test = patches_from_original_test_split()\n",
    "\n",
    "    if patch in train:\n",
    "        return \"train\"\n",
    "    elif patch in validation:\n",
    "        return \"validation\"\n",
    "    elif patch in test:\n",
    "        return \"test\"\n",
    "    warnings.warn(\n",
    "        \"Provided an input patch name which was not part of the original split.\",\n",
    "        UserWarning,\n",
    "    )\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert \"train\" == get_original_split_from_patch_name(\"S2A_MSIL2A_20170717T113321_28_87\")\n",
    "assert \"validation\" == get_original_split_from_patch_name(\n",
    "    \"S2B_MSIL2A_20170812T092029_75_6\"\n",
    ")\n",
    "assert \"test\" == get_original_split_from_patch_name(\"S2A_MSIL2A_20170717T113321_28_88\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    get_original_split_from_patch_name(\"WrongName\")\n",
    "    assert len(w) == 1\n",
    "    assert issubclass(w[-1].category, UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def _old2new_label(old_label: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Converts old-style BigEearthNet label to the\n",
    "    new labels.\n",
    "\n",
    "    > Note: Some labels were removed! This function\n",
    "    will return `None` if the label was removed and\n",
    "    raise a `KeyError` if the input label is unknown.\n",
    "    \"\"\"\n",
    "    return OLD2NEW_LABELS_DICT[old_label]\n",
    "\n",
    "\n",
    "def old2new_labels(old_labels: Sequence[str]) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Converts a list of old-style BigEarthNet labels\n",
    "    to a list of labels.\n",
    "\n",
    "    If there are no corresponding new labels (which can happen with original BEN patches!)\n",
    "    then the function will return `None` and raise a user warning.\n",
    "\n",
    "    If an illegal/unknown input label is provided, a `KeyError` is raised.\n",
    "    \"\"\"\n",
    "    new_labels = [\n",
    "        _old2new_label(l) for l in old_labels if _old2new_label(l) is not None\n",
    "    ]\n",
    "    if len(old_labels) > 0 and len(new_labels) == 0:\n",
    "        warnings.warn(\n",
    "            \"Provided a list of old labels that only contains `removed` labels!\",\n",
    "            UserWarning,\n",
    "        )\n",
    "        new_labels = None\n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Some of the original 43-class nomenclature patches have 0 labels with the 19-class nomenclature! This function might return `None` instead of an empty list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    old2new_labels([\"Burnt areas\"])\n",
    "    assert len(w) == 1\n",
    "    assert issubclass(w[-1].category, UserWarning)\n",
    "\n",
    "old2new_labels((\"Burnt areas\",))\n",
    "assert len(w) == 1\n",
    "\n",
    "with fc.ExceptionExpected(ex=KeyError):\n",
    "    old2new_labels([\"Illegal input label\"])\n",
    "\n",
    "fc.test_eq(\n",
    "    old2new_labels(\n",
    "        [\n",
    "            \"Continuous urban fabric\",\n",
    "            \"Discontinuous urban fabric\",\n",
    "        ]\n",
    "    ),\n",
    "    [\"Urban fabric\", \"Urban fabric\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def ben_19_labels_to_multi_hot(labels: Sequence[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Convenience function that converts an input sequence of labels into\n",
    "    a multi-hot encoded vector.\n",
    "    The naturally ordered label list is used as an encoder reference\n",
    "    - `bigearthnet_common.NEW_LABELS`\n",
    "\n",
    "    If an unknown label is given, a `KeyError` is raised.\n",
    "    \n",
    "    Be aware that this approach assumes that **all** labels are actually used in the dataset!\n",
    "    This is not necessarily the case if you are using a subset!\n",
    "    For example, the \"Agro-forestry areas\" class is only present in Portugal and in no other country!\n",
    "    \"\"\"\n",
    "    idxs = [ben_constants.NEW_LABELS_TO_IDX[label] for label in labels]\n",
    "    multi_hot = fc.L([0] * len(ben_constants.NEW_LABELS))\n",
    "    multi_hot[idxs] = 1.\n",
    "    return list(multi_hot)\n",
    "\n",
    "@validate_arguments\n",
    "def ben_43_labels_to_multi_hot(labels: Sequence[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Convenience function that converts an input sequence of labels into\n",
    "    a multi-hot encoded vector.\n",
    "    The naturally ordered label list is used as an encoder reference\n",
    "    - `bigearthnet_common.OLD_LABELS`\n",
    "\n",
    "    If an unknown label is given, a `KeyError` is raised.\n",
    "    \n",
    "    Be aware that this approach assumes that **all** labels are actually used in the dataset!\n",
    "    This is not necessarily the case if you are using a subset!\n",
    "    For example, the \"Agro-forestry areas\" class is only present in Portugal and in no other country!\n",
    "    \"\"\"\n",
    "    idxs = [ben_constants.OLD_LABELS_TO_IDX[label] for label in labels]\n",
    "    multi_hot = fc.L([0] * len(ben_constants.OLD_LABELS))\n",
    "    multi_hot[idxs] = 1.\n",
    "    return list(multi_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agro = ben_19_labels_to_multi_hot((\"Agro-forestry areas\", ))\n",
    "assert len(agro) == 19\n",
    "assert agro[0] == 1.0\n",
    "assert not any(agro[1:])\n",
    "\n",
    "multi = ben_19_labels_to_multi_hot((\"Agro-forestry areas\", \"Arable land\"))\n",
    "assert multi[0] == 1.0\n",
    "assert multi[1] == 1.0\n",
    "assert not any(multi[2:])\n",
    "\n",
    "with fc.ExceptionExpected(ex=KeyError):\n",
    "    ben_19_labels_to_multi_hot([\"Airports\"])\n",
    "\n",
    "agro43 = ben_43_labels_to_multi_hot((\"Agro-forestry areas\", ))\n",
    "assert len(agro43) == 43\n",
    "assert agro43[0] == 1.0\n",
    "assert not any(agro43[1:])\n",
    "\n",
    "multi43 = ben_43_labels_to_multi_hot((\"Agro-forestry areas\", \"Airports\"))\n",
    "assert multi43[0] == 1.0\n",
    "assert multi43[1] == 1.0\n",
    "assert not any(multi43[2:])\n",
    "\n",
    "with fc.ExceptionExpected(ex=KeyError):\n",
    "    ben_43_labels_to_multi_hot([\"Arable land\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01a_constants.ipynb.\n",
      "Converted 01b_base.ipynb.\n",
      "Converted 01c_gdf_builder.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /home/kai/git/bigearthnet_common/nbs/01a_constants.ipynb\n",
      "converting: /home/kai/git/bigearthnet_common/nbs/01b_base.ipynb\n",
      "converting /home/kai/git/bigearthnet_common/nbs/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.cli import nbdev_build_docs\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n",
    "nbdev_build_docs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
