{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp base\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigEarthNet Base Functions\n",
    "> A collection of common function that are applied to BEN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import bz2\n",
    "import csv\n",
    "import functools\n",
    "import json\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "from importlib import resources\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Iterable, Set, Union\n",
    "import rich\n",
    "import typer\n",
    "\n",
    "import appdirs\n",
    "import dateutil\n",
    "import fastcore.all as fc\n",
    "from fastcore.basics import compose\n",
    "from fastcore.dispatch import typedispatch\n",
    "from pydantic import DirectoryPath, FilePath, validate_arguments\n",
    "\n",
    "import bigearthnet_common\n",
    "import bigearthnet_common.constants as ben_constants\n",
    "\n",
    "_patches_with_cloud_and_snow_resource = \"patches_with_cloud_and_shadow.csv.bz2\"\n",
    "_patches_with_seasonal_snow_resource = \"patches_with_seasonal_snow.csv.bz2\"\n",
    "_patches_with_no_19_class_targets = \"patches_with_no_19_class_targets.csv.bz2\"\n",
    "_s1_s2_mapping_resource = \"s1_s2_name_country_season.csv.bz2\"\n",
    "_country_resource = \"s1_s2_name_country_season.csv.bz2\"\n",
    "_season_resource = \"s1_s2_name_country_season.csv.bz2\"\n",
    "_test_resource = \"test.csv.bz2\"\n",
    "_train_csv_resource = \"train.csv.bz2\"\n",
    "_val_csv_resource = \"val.csv.bz2\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "USER_DIR = Path(appdirs.user_data_dir(\"bigearthnet\"))\n",
    "USER_DIR.mkdir(exist_ok=True, parents=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# Check if resources exist\n",
    "assert resources.is_resource(bigearthnet_common, _patches_with_cloud_and_snow_resource)\n",
    "assert resources.is_resource(bigearthnet_common, _patches_with_seasonal_snow_resource)\n",
    "assert resources.is_resource(bigearthnet_common, _s1_s2_mapping_resource)\n",
    "assert resources.is_resource(bigearthnet_common, _test_resource)\n",
    "assert resources.is_resource(bigearthnet_common, _train_csv_resource)\n",
    "assert resources.is_resource(bigearthnet_common, _val_csv_resource)\n",
    "assert resources.is_resource(bigearthnet_common, _country_resource)\n",
    "assert resources.is_resource(bigearthnet_common, _season_resource)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "from datetime import date\n",
    "\n",
    "from dateutil.parser import ParserError\n",
    "from fastcore.test import ExceptionExpected, test_eq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# from pydantic import AnyHttpUrl\n",
    "# import urllib\n",
    "# @validate_arguments\n",
    "# def _download_and_cache_url(url: AnyHttpUrl, force_download: bool = False):\n",
    "#     \"\"\"\n",
    "#     Simply download contents of url to the default user directory.\n",
    "#     Allow to redownload with `force_download`\n",
    "#     \"\"\"\n",
    "#     fp = USER_DIR / Path(url).name\n",
    "#     if not fp.exists() or force_download:\n",
    "#         response = urllib.request.urlopen(url).read()\n",
    "#         fp.write_bytes(response)\n",
    "#     return fp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Safe JSON Parsing\n",
    "\n",
    "A couple of safe parsing functions that guarantee flexible reading of the default BigEarthNet json entries.\n",
    "\n",
    "Usually, you should not use any of these parsing functions directly, but use one of the higher-level functions instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def parse_datetime(inp: Union[str, datetime]) -> datetime:\n",
    "    \"\"\"\n",
    "    Parses an input into a `datetime` object.\n",
    "    Will try its best to infer the correct format from a string.\n",
    "    If a `datetime` object is already provided it will be returned.\n",
    "    Otherwise it will raise an error.\n",
    "    \"\"\"\n",
    "    return _parse_datetime(inp)\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def _parse_datetime(acquisition_date: str) -> datetime:\n",
    "    return compose(dateutil.parser.parse, _parse_datetime)(acquisition_date)\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def _parse_datetime(acquisition_date: datetime) -> datetime:\n",
    "    return acquisition_date\n",
    "\n",
    "\n",
    "@typedispatch\n",
    "def _parse_datetime(acquisition_date: object) -> None:\n",
    "    raise TypeError(\"Could not parse acquisition_date!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = parse_datetime(\"2017-06-13 10:10:31\")\n",
    "d2 = parse_datetime(\"13.06.2017 10:10:31\")\n",
    "d3 = parse_datetime(datetime(year=2017, month=6, day=13, hour=10, minute=10, second=31))\n",
    "\n",
    "test_eq(d1, d2)\n",
    "test_eq(d2, d3)\n",
    "\n",
    "with ExceptionExpected(ex=ParserError, regex=\"format\"):\n",
    "    parse_datetime(\"large_tile\")\n",
    "\n",
    "with ExceptionExpected(ex=TypeError, regex=\"parse\"):\n",
    "    parse_datetime(42)\n",
    "\n",
    "with ExceptionExpected(ex=TypeError, regex=\"parse\"):\n",
    "    parse_datetime(date(year=2017, month=10, day=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def _read_json(\n",
    "    json_fp: FilePath, expected_keys: Set, read_only_expected: bool = True\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parse the json file given with the file path `json_fp`.\n",
    "    The function checks if all of the `expected_keys` are present, which\n",
    "    ensures that no keys have been accidentilly deleted (this has happend before).\n",
    "    If `read_only_expected` is set, only the keys provided in `expected_keys` are read\n",
    "    and returned.\n",
    "    This prevents accidental processing of injected metadata.\n",
    "\n",
    "    Args:\n",
    "        json_fp (FilePath): Path to json file\n",
    "        expected_keys (Set): Keys that are expected to be present in the json file\n",
    "        read_only_expected (bool, optional): Read only the keys given in `expected_keys`. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        [Dict[str, str]]: A dictionary of the keys.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        complete_data = json.loads(json_fp.read_text())\n",
    "    except json.JSONDecodeError:\n",
    "        raise ValueError(f\"Error trying to read json from: \", json_fp)\n",
    "\n",
    "    missing_elements = expected_keys - complete_data.keys()\n",
    "    if len(missing_elements) > 0:\n",
    "        raise ValueError(f\"{json_fp} is missing entries!\", missing_elements)\n",
    "\n",
    "    # ensure that the original values are loaded, as some users may customize the original json files\n",
    "    if read_only_expected:\n",
    "        return {k: v for k, v in complete_data.items() if k in expected_keys}\n",
    "    return complete_data\n",
    "\n",
    "\n",
    "def read_S1_json(json_fp: FilePath) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    A helper function that *safely* reads a BigEarthNet-S1 json file.\n",
    "    It will ensure that all expected entries are present and only read those\n",
    "    entries.\n",
    "    This helps to avoid issues where the JSON files were accidentally modified\n",
    "    or partially deleted.\n",
    "\n",
    "    Note: This function will also silently fix a typo present in the `coordinates` key\n",
    "    from version: S1_v1.0. A coordinates key is named `lly` and it should be `lry`.\n",
    "    \"\"\"\n",
    "    data = _read_json(json_fp, ben_constants.BEN_S1_V1_0_JSON_KEYS)\n",
    "    # Silently fix key error in S1\n",
    "    if \"lly\" in data[\"coordinates\"]:\n",
    "        data[\"coordinates\"][\"lry\"] = data[\"coordinates\"].pop(\"lly\")\n",
    "    return data\n",
    "\n",
    "\n",
    "def read_S2_json(json_fp: FilePath) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    A helper function that *safely* reads a BigEarthNet-S1 json file.\n",
    "    It will ensure that all expected entries are present and only read those\n",
    "    entries.\n",
    "    This helps to avoid issues where the JSON files were accidentally modified\n",
    "    or partially deleted.\n",
    "    \"\"\"\n",
    "    return _read_json(json_fp, ben_constants.BEN_S2_V1_0_JSON_KEYS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "s2_json_path = \"S2_json_only/S2A_MSIL2A_20170617T113321_4_55/S2A_MSIL2A_20170617T113321_4_55_labels_metadata.json\"\n",
    "s2_data = read_S2_json(s2_json_path)\n",
    "assert all(k in ben_constants.BEN_S2_V1_0_JSON_KEYS for k in s2_data)\n",
    "assert len(s2_data) == len(ben_constants.BEN_S2_V1_0_JSON_KEYS)\n",
    "\n",
    "s1_json_path = \"S1_json_only/S1A_IW_GRDH_1SDV_20170613T165043_33UUP_61_39/S1A_IW_GRDH_1SDV_20170613T165043_33UUP_61_39_labels_metadata.json\"\n",
    "s1_data = read_S1_json(s1_json_path)\n",
    "assert all(k in ben_constants.BEN_S1_V1_0_JSON_KEYS for k in s1_data)\n",
    "assert len(s1_data) == len(ben_constants.BEN_S1_V1_0_JSON_KEYS)\n",
    "\n",
    "with ExceptionExpected(ValueError, \"missing entries\"):\n",
    "    read_S2_json(s1_json_path)\n",
    "\n",
    "with ExceptionExpected(ValueError, \"missing entries\"):\n",
    "    read_S1_json(s2_json_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common BEN patch checks and transformations\n",
    "\n",
    "To quickly filter a list of directories and ensure that only Sentinel directories are accessed, use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def get_s2_patch_directories(dir_path: DirectoryPath) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Will find all S2 patch directories in the provided `dir_path`.\n",
    "    Only directories that strictly cohere to the naming convention will be returned.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        p\n",
    "        for p in dir_path.iterdir()\n",
    "        if ben_constants.BEN_S2_RE.fullmatch(p.name) is not None\n",
    "    ]\n",
    "\n",
    "\n",
    "@validate_arguments\n",
    "def get_s1_patch_directories(dir_path: DirectoryPath) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Will find all S1 patch directories in the provided `dir_path`.\n",
    "    Only directories that strictly cohere to the naming convention will be returned.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        p\n",
    "        for p in dir_path.iterdir()\n",
    "        if ben_constants.BEN_S1_RE.fullmatch(p.name) is not None\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "s2_dir = \"S2_json_only\"\n",
    "s1_dir = \"S1_json_only\"\n",
    "\n",
    "assert len(get_s2_patch_directories(s2_dir)) == 2\n",
    "assert len(get_s2_patch_directories(s1_dir)) == 0\n",
    "\n",
    "assert len(get_s1_patch_directories(s1_dir)) == 2\n",
    "assert len(get_s1_patch_directories(s2_dir)) == 0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions mainly allow the user to write cleaner code by importing them instead of writing lambda functions.\n",
    "All functions use caches to guarantee fast lookups, so feel free to use them on large data.\n",
    "\n",
    "The most relevant functions are:\n",
    "- check if patch name in cloud/snow collection\n",
    "    - `is_snowy_patch`\n",
    "    - `is_cloudy_shadowy_patch`\n",
    "- Retrieve the original split by looking up the patch name\n",
    "    - `get_original_split_from_patch_name`\n",
    "- Convert the old 43-label nomenclature to the new 19-label variant\n",
    "    - `old2new_labels`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "PATCHES_WITH_SNOW_URL = (\n",
    "    \"http://bigearth.net/static/documents/patches_with_seasonal_snow.csv\"\n",
    ")\n",
    "PATCHES_WITH_CLOUD_AND_SHADOW_URL = (\n",
    "    \"http://bigearth.net/static/documents/get_patches_with_cloud_and_shadow.csv\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@functools.lru_cache()\n",
    "def _conv_header_col_bz2_csv_resource_to_dict(\n",
    "    resource, key_column: str, value_column: str\n",
    ") -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load a dictionary with the provided `key_column` and `value_column` after uncompressing\n",
    "    the `bz2` compressed `resource` csv file.\n",
    "    \"\"\"\n",
    "    if not resources.is_resource(bigearthnet_common, resource):\n",
    "        raise ValueError(\n",
    "            f\"{resource} resource is not available! This means that it was forgotten to be packaged.\"\n",
    "        )\n",
    "\n",
    "    with resources.path(bigearthnet_common, resource) as resource_path:\n",
    "        with bz2.open(resource_path, mode=\"rt\") as csv_file:\n",
    "            reader = csv.DictReader(\n",
    "                csv_file\n",
    "            )  # field-names are encoded as first csv row\n",
    "            if key_column not in reader.fieldnames:\n",
    "                raise ValueError(\n",
    "                    f\"Key {key_column} is unkown! Resource provides: {reader.fieldnames}\"\n",
    "                )\n",
    "            if value_column not in reader.fieldnames:\n",
    "                raise ValueError(\n",
    "                    f\"Value {value_column} is unkown! Resource provides: {reader.fieldnames}\"\n",
    "                )\n",
    "            return {row[key_column]: row[value_column] for row in reader}\n",
    "\n",
    "\n",
    "# FUTURE: Could merge the underlying logic together\n",
    "@functools.lru_cache()\n",
    "def _conv_header_col_bz2_csv_resource_to_set(\n",
    "    resource, key_column: str\n",
    ") -> Set[str]:\n",
    "    \"\"\"\n",
    "    Load a dictionary with the provided `key_column` after uncompressing\n",
    "    the `bz2` compressed `resource` csv file.\n",
    "    \"\"\"\n",
    "    if not resources.is_resource(bigearthnet_common, resource):\n",
    "        raise ValueError(\n",
    "            f\"{resource} resource is not available! This means that it was forgotten to be packaged.\"\n",
    "        )\n",
    "\n",
    "    with resources.path(bigearthnet_common, resource) as resource_path:\n",
    "        with bz2.open(resource_path, mode=\"rt\") as csv_file:\n",
    "            reader = csv.DictReader(\n",
    "                csv_file\n",
    "            )  # field-names are encoded as first csv row\n",
    "            if key_column not in reader.fieldnames:\n",
    "                raise ValueError(\n",
    "                    f\"Key {key_column} is unkown! Resource provides: {reader.fieldnames}\"\n",
    "                )\n",
    "            return {row[key_column] for row in reader}\n",
    "\n",
    "def get_all_s2_patch_names() -> Set[str]:\n",
    "    resource = _s1_s2_mapping_resource\n",
    "    return _conv_header_col_bz2_csv_resource_to_set(resource, \"s2_name\")\n",
    "\n",
    "def get_all_s1_patch_names() -> Set[str]:\n",
    "    resource = _s1_s2_mapping_resource\n",
    "    return _conv_header_col_bz2_csv_resource_to_set(resource, \"s1_name\")\n",
    "\n",
    "def _load_s1_s2_patch_name_mapping(from_s1_to_s2: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load a dictionary which maps the S1 patch name to the S2 patch name (if `from_s1_to_s2`) or\n",
    "    the S2 patch name to the S1 patch name.\n",
    "\n",
    "    The compressed data could be regenerated with (requires the output of `bigearthnet_gdf_builder`):\n",
    "    >>> import geopandas\n",
    "    >>> raw_gdf = geopandas.read_parquet(\"raw_ben_s1_gdf.parquet\")\n",
    "    >>> raw_gdf = raw_gdf.rename({\"name\": \"s1_name\", \"corresponding_s2_patch\": \"s2_name\"}, axis=1)\n",
    "    >>> raw_gdf.to_csv(\"s1_s2_mapping.csv.bz2\", columns=[\"s1_name\", \"s2_name\"], index=False)\n",
    "    \"\"\"\n",
    "    resource = _s1_s2_mapping_resource\n",
    "    key, value = (\"s1_name\", \"s2_name\") if from_s1_to_s2 else (\"s2_name\", \"s1_name\")\n",
    "    return _conv_header_col_bz2_csv_resource_to_dict(resource, key, value)\n",
    "\n",
    "\n",
    "def get_complete_s1_to_s2_patch_name_mapping() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load entire Sentinel-1 to Sentinel-2 BigEarthNet patch name mapping.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Sentinel-1 patch name keys with corresponding Sentinel-2 patch name as value\n",
    "    \"\"\"\n",
    "    return _load_s1_s2_patch_name_mapping(from_s1_to_s2=True)\n",
    "\n",
    "\n",
    "def get_complete_s2_to_s1_patch_name_mapping() -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Load entire Sentinel-2 to Sentinel-1 BigEarthNet patch name mapping.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, str]: Sentinel-2 patch name keys with corresponding Sentinel-1 patch name as value\n",
    "    \"\"\"\n",
    "    return _load_s1_s2_patch_name_mapping(from_s1_to_s2=False)\n",
    "\n",
    "\n",
    "def s1_to_s2_patch_name(s1_patch_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert BigEarthNet Sentinel-1 patch name to Sentinel-2 patch name.\n",
    "    The function caches intermediate results.\n",
    "    The function should be highly performant.\n",
    "\n",
    "    Args:\n",
    "        s1_patch_name (str): complete BigEarthNet Sentinel-1 patch name\n",
    "\n",
    "    Returns:\n",
    "        str: Corresponding Sentinel-2 patch name\n",
    "    \"\"\"\n",
    "    return get_complete_s1_to_s2_patch_name_mapping()[s1_patch_name]\n",
    "\n",
    "\n",
    "def s2_to_s1_patch_name(s2_patch_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Convert BigEarthNet Sentinel-2 patch name to Sentinel-1 patch name.\n",
    "    The function caches intermediate results.\n",
    "    The function should be highly performant.\n",
    "\n",
    "    Args:\n",
    "        s2_patch_name (str): complete BigEarthNet Sentinel-2 patch name\n",
    "\n",
    "    Returns:\n",
    "        str: Corresponding Sentinel-1 patch name\n",
    "    \"\"\"\n",
    "    return get_complete_s2_to_s1_patch_name_mapping()[s2_patch_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "s1_patches = get_all_s1_patch_names()\n",
    "s2_patches = get_all_s2_patch_names()\n",
    "s1_to_s2_mapping = get_complete_s1_to_s2_patch_name_mapping()\n",
    "s2_to_s1_mapping = get_complete_s2_to_s1_patch_name_mapping()\n",
    "assert s1_patches == set(s1_to_s2_mapping.keys())\n",
    "assert s2_patches == set(s2_to_s1_mapping.keys())\n",
    "\n",
    "assert all(k == v for k, v in zip(s1_to_s2_mapping.keys(), s2_to_s1_mapping.values()))\n",
    "assert all(k == v for k, v in zip(s1_to_s2_mapping.values(), s2_to_s1_mapping.keys()))\n",
    "assert all(\n",
    "    ben_constants.BEN_S1_RE.fullmatch(s1_patch) for s1_patch in s1_to_s2_mapping.keys()\n",
    ")\n",
    "assert all(\n",
    "    ben_constants.BEN_S2_RE.fullmatch(s2_patch)\n",
    "    for s2_patch in s1_to_s2_mapping.values()\n",
    ")\n",
    "assert len(s1_to_s2_mapping) == ben_constants.BEN_COMPLETE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_patches_to_country_mapping(use_s2_patch_names: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return a dictionary that maps a patch name to a country.\n",
    "    If `use_s2_patch_names` is set, use the BigEarthNet Sentinel-2 patch names.\n",
    "    Otherwise, use the Sentinel-1 patch names.\n",
    "\n",
    "    The compressed data could be regenerated with (requires the output of `bigearthnet_gdf_builder`):\n",
    "    >>> import geopandas\n",
    "    >>> extended_gdf = geopandas.read_parquet(\"extended_ben_s1_gdf.parquet\")\n",
    "    >>> extended_gdf = raw_gdf.rename({\"name\": \"s1_name\", \"corresponding_s2_patch\": \"s2_name\"}, axis=1)\n",
    "    >>> extended_gdf.to_csv(\"country.csv.bz2\", columns=[\"s1_name\", \"s2_name\", \"country\"], index=False)\n",
    "    \"\"\"\n",
    "    resource = _country_resource\n",
    "    key = \"s2_name\" if use_s2_patch_names else \"s1_name\"\n",
    "    return _conv_header_col_bz2_csv_resource_to_dict(resource, key, \"country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_country_mapping = get_patches_to_country_mapping()\n",
    "s1_country_mapping = get_patches_to_country_mapping(use_s2_patch_names=False)\n",
    "assert len(s1_country_mapping) == len(s2_country_mapping)\n",
    "assert all(\n",
    "    s1_value == s2_value\n",
    "    for s1_value, s2_value in zip(\n",
    "        s1_country_mapping.values(), s2_country_mapping.values()\n",
    "    )\n",
    ")\n",
    "\n",
    "# validate that countries are only expected countries\n",
    "assert set(s2_country_mapping.values()) == set(ben_constants.COUNTRIES)\n",
    "# check by visual inspection\n",
    "assert s2_country_mapping[\"S2A_MSIL2A_20171221T112501_56_35\"] == \"Portugal\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_patches_to_season_mapping(use_s2_patch_names: bool = True) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Return a dictionary that maps a patch name to the season of the acquisition date.\n",
    "    If `use_s2_patch_names` is set, use the BigEarthNet Sentinel-2 patch names.\n",
    "    Otherwise, use the Sentinel-1 patch names.\n",
    "\n",
    "    The compressed data could be regenerated with (requires the output of `bigearthnet_gdf_builder`):\n",
    "    >>> import geopandas\n",
    "    >>> extended_gdf = geopandas.read_parquet(\"extended_ben_s1_gdf.parquet\")\n",
    "    >>> extended_gdf = raw_gdf.rename({\"name\": \"s1_name\", \"corresponding_s2_patch\": \"s2_name\"}, axis=1)\n",
    "    >>> extended_gdf.to_csv(\"season.csv.bz2\", columns=[\"s1_name\", \"s2_name\", \"season\"], index=False)\n",
    "    \"\"\"\n",
    "    resource = _season_resource\n",
    "    key = \"s2_name\" if use_s2_patch_names else \"s1_name\"\n",
    "    return _conv_header_col_bz2_csv_resource_to_dict(resource, key, \"season\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_season_mapping = get_patches_to_season_mapping()\n",
    "s1_season_mapping = get_patches_to_season_mapping(use_s2_patch_names=False)\n",
    "assert len(s1_season_mapping) == len(s2_season_mapping)\n",
    "assert all(\n",
    "    s1_value == s2_value\n",
    "    for s1_value, s2_value in zip(\n",
    "        s1_season_mapping.values(), s2_season_mapping.values()\n",
    "    )\n",
    ")\n",
    "\n",
    "# There are only 4 seasons\n",
    "assert len(set(s2_season_mapping.values())) == 4\n",
    "assert set(s2_season_mapping.values()) == set(s for s in ben_constants.Season)\n",
    "# check by inspecting the acquisition time\n",
    "assert s2_season_mapping[\"S2A_MSIL2A_20171221T112501_56_35\"] == ben_constants.Season.Winter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def _conv_single_col_csv_resource_to_set(\n",
    "    resource: str,\n",
    ") -> Set[str]:\n",
    "    \"\"\"\n",
    "    Given a `resource` name of an encoded CSV file *without* a header\n",
    "    line and only a single column, return the set of\n",
    "    all values.\n",
    "    \"\"\"\n",
    "    if not resources.is_resource(bigearthnet_common, resource):\n",
    "        raise ValueError(f\"{resource} is an unknown resource!\")\n",
    "\n",
    "    with resources.path(bigearthnet_common, resource) as resource_path:\n",
    "        with bz2.open(resource_path, mode=\"rt\") as csv_file:\n",
    "            col_name = \"Column\"\n",
    "            reader = csv.DictReader(csv_file, fieldnames=[col_name])\n",
    "            return {row[col_name] for row in reader}\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s2_patches_with_seasonal_snow() -> Set[str]:\n",
    "    \"\"\"List all patches with seasonal snow from **original** BigEarthNet-S2 dataset.\"\"\"\n",
    "    return _conv_single_col_csv_resource_to_set(_patches_with_seasonal_snow_resource)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s2_patches_with_cloud_and_shadow() -> Set[str]:\n",
    "    \"\"\"List all patches with cloud and shadow from **original** BigEarthNet-S2 dataset.\"\"\"\n",
    "    return _conv_single_col_csv_resource_to_set(_patches_with_cloud_and_snow_resource)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s1_patches_with_seasonal_snow() -> Set[str]:\n",
    "    \"\"\"List all patches with seasonal snow from **original** BigEarthNet-S1 dataset.\"\"\"\n",
    "    _s2_patches_with_snow = get_s2_patches_with_seasonal_snow()\n",
    "    return {s2_to_s1_patch_name(s2_patch) for s2_patch in _s2_patches_with_snow}\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s1_patches_with_cloud_and_shadow() -> Set[str]:\n",
    "    \"\"\"List all patches with cloud and shadow from **original** BigEarthNet-S1 dataset.\"\"\"\n",
    "    _s2_patches_with_clouds = get_s2_patches_with_cloud_and_shadow()\n",
    "    return {s2_to_s1_patch_name(s2_patch) for s2_patch in _s2_patches_with_clouds}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2_snow_patches = get_s2_patches_with_seasonal_snow()\n",
    "assert len(s2_snow_patches) == ben_constants.BEN_SNOWY_PATCHES_COUNT\n",
    "s1_snow_patches = get_s1_patches_with_seasonal_snow()\n",
    "assert len(s1_snow_patches) == ben_constants.BEN_SNOWY_PATCHES_COUNT\n",
    "\n",
    "s2_cloud_and_shadow_patches = get_s2_patches_with_cloud_and_shadow()\n",
    "assert (\n",
    "    len(s2_cloud_and_shadow_patches)\n",
    "    == ben_constants.BEN_CLOUDY_OR_SHADOWY_PATCHES_COUNT\n",
    ")\n",
    "s1_cloud_and_shadow_patches = get_s1_patches_with_cloud_and_shadow()\n",
    "assert (\n",
    "    len(s1_cloud_and_shadow_patches)\n",
    "    == ben_constants.BEN_CLOUDY_OR_SHADOWY_PATCHES_COUNT\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def is_snowy_patch(patch_name: str):\n",
    "    \"\"\"\n",
    "    Fast function that checks whether `patch_name` is a patch\n",
    "    that contains a lot of seasonal snow.\n",
    "\n",
    "    This works for S1 and S2 patch names!\n",
    "    \"\"\"\n",
    "    return (\n",
    "        patch_name in get_s2_patches_with_seasonal_snow()\n",
    "        or patch_name in get_s1_patches_with_seasonal_snow()\n",
    "    )\n",
    "\n",
    "\n",
    "@validate_arguments\n",
    "def is_cloudy_shadowy_patch(patch_name: str):\n",
    "    \"\"\"\n",
    "    Fast function that checks whether `patch_name` is a patch\n",
    "    that contains a lot of shadow or is obstructed by clouds.\n",
    "\n",
    "    This works for S1 and S2 patch names!\n",
    "    \"\"\"\n",
    "    return (\n",
    "        patch_name in get_s2_patches_with_cloud_and_shadow()\n",
    "        or patch_name in get_s1_patches_with_cloud_and_shadow()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "\n",
    "# doesn't check if name is sensible\n",
    "assert is_snowy_patch(\"hello\") == False\n",
    "\n",
    "assert is_snowy_patch(\"S2A_MSIL2A_20180205T100211_2_0\") == True\n",
    "assert is_snowy_patch(\"S1A_IW_GRDH_1SDV_20180417T155012_34WFV_59_20\") == True\n",
    "\n",
    "assert is_snowy_patch(\"S2B_MSIL2A_20170906T101019_33_85\") == False\n",
    "assert is_snowy_patch(\"S1A_IW_GRDH_1SDV_20170904T161304_34VDN_33_85\") == False\n",
    "\n",
    "assert is_cloudy_shadowy_patch(\"hello\") == False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@functools.lru_cache()\n",
    "def get_s2_patches_with_no_19_class_target() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all patches from the BigEarthNet-S2 dataset that\n",
    "    have _no_ defined classes with the 19-class nomenclature.\n",
    "\n",
    "    Note: This set still includes patches with snow, clouds, or shadows.\n",
    "\n",
    "    To re-build the file, it is necessary to use the output of `bigearthnet_gdf_builder`:\n",
    "    >>> raw_gdf = geopandas.read_parquet(\"raw_ben_s2_gdf.parquet\")\n",
    "    >>> raw_gdf[\"new_labels\"] = raw_gdf[\"labels\"].apply(old2new_labels)\n",
    "    >>> no_19_label_targets = raw_gdf[raw_gdf[\"new_labels\"].isna()]\n",
    "    >>> no_19_label_targets.to_csv(\"patches_with_no_19_class_targets.csv.bz2\", columns=[\"name\"], index=False, header=False)\n",
    "    \"\"\"\n",
    "    return _conv_single_col_csv_resource_to_set(_patches_with_no_19_class_targets)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s1_patches_with_no_19_class_target() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all patches from the BigEarthNet-S1 dataset that\n",
    "    have _no_ defined classes with the 19-class nomenclature.\n",
    "\n",
    "    Note: This set still includes patches with snow, clouds, or shadows.\n",
    "\n",
    "    The patch names are converted from `get_s2_patches_with_no_19_class_targets`\n",
    "    for compactness.\n",
    "    \"\"\"\n",
    "    s2_patches_no_19_classes = _conv_single_col_csv_resource_to_set(\n",
    "        _patches_with_no_19_class_targets\n",
    "    )\n",
    "    return {s2_to_s1_patch_name(s2_patch) for s2_patch in s2_patches_no_19_classes}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "assert (\n",
    "    len(get_s2_patches_with_no_19_class_target())\n",
    "    == ben_constants.BEN_NO_19_CLASS_TARGET_COUNT\n",
    ")\n",
    "assert (\n",
    "    len(get_s1_patches_with_no_19_class_target())\n",
    "    == ben_constants.BEN_NO_19_CLASS_TARGET_COUNT\n",
    ")\n",
    "\n",
    "# contains a single patch which is snowy\n",
    "s2_patches_no_19_class = get_s2_patches_with_no_19_class_target()\n",
    "no_snowy = {p for p in s2_patches_no_19_class if not is_snowy_patch(p)}\n",
    "assert len(no_snowy) < len(s2_patches_no_19_class)\n",
    "\n",
    "# contains a single patch which is cloudy\n",
    "s2_patches_no_19_class = get_s2_patches_with_no_19_class_target()\n",
    "no_clouds = {p for p in s2_patches_no_19_class if not is_cloudy_shadowy_patch(p)}\n",
    "assert len(no_clouds) < len(s2_patches_no_19_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "\n",
    "# FUTURE: Remove this bz2 file and repackage it inside of the\n",
    "# metadata collection file\n",
    "# \"https://git.tu-berlin.de/rsim/BigEarthNet-S2_19-classes_models/-/raw/master/splits/train.csv\",\n",
    "@functools.lru_cache()\n",
    "def get_s2_patches_from_original_train_split() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all Sentinel-2 train patches from the original train/validation/test split.\n",
    "    \"\"\"\n",
    "    return _conv_single_col_csv_resource_to_set(_train_csv_resource)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s1_patches_from_original_train_split() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all Sentinel-1 train patches from the original train/validation/test split.\n",
    "    \"\"\"\n",
    "    s2_train_patches = get_s2_patches_from_original_train_split()\n",
    "    return {s2_to_s1_patch_name(s2_patch) for s2_patch in s2_train_patches}\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s2_patches_from_original_validation_split() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all Sentinel-2 validation patches from the original train/validation/test split.\n",
    "    \"\"\"\n",
    "    return _conv_single_col_csv_resource_to_set(_val_csv_resource)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s1_patches_from_original_validation_split() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all Sentinel-1 validation patches from the original train/validation/test split.\n",
    "    \"\"\"\n",
    "    s2_validation_patches = get_s2_patches_from_original_validation_split()\n",
    "    return {s2_to_s1_patch_name(s2_patch) for s2_patch in s2_validation_patches}\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s2_patches_from_original_test_split() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all Sentinel-2 test patches from the original train/validation/test split.\n",
    "    \"\"\"\n",
    "    return _conv_single_col_csv_resource_to_set(_test_resource)\n",
    "\n",
    "\n",
    "@functools.lru_cache()\n",
    "def get_s1_patches_from_original_test_split() -> Set[str]:\n",
    "    \"\"\"\n",
    "    List all Sentinel-1 test patches from the original train/validation/test split.\n",
    "    \"\"\"\n",
    "    s2_test_patches = get_s2_patches_from_original_test_split()\n",
    "    return {s2_to_s1_patch_name(s2_patch) for s2_patch in s2_test_patches}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "s2_train = get_s2_patches_from_original_train_split()\n",
    "s1_train = get_s1_patches_from_original_train_split()\n",
    "\n",
    "s2_val = get_s2_patches_from_original_validation_split()\n",
    "s1_val = get_s1_patches_from_original_validation_split()\n",
    "\n",
    "s2_test = get_s2_patches_from_original_test_split()\n",
    "s1_test = get_s1_patches_from_original_test_split()\n",
    "\n",
    "assert len(s2_test) < len(s2_train)\n",
    "assert len(s2_val) < len(s2_train)\n",
    "assert len(s1_train) == len(s2_train)\n",
    "assert len(s1_val) == len(s2_val)\n",
    "assert len(s1_test) == len(s2_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def get_original_split_from_patch_name(patch: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Returns \"train\"/\"validation\"/\"test\" or `None`.\n",
    "    The value is retrieved from the original BigEarthNet-S1/S2\n",
    "    train/validation/test split. If the input is not present\n",
    "    in any split, it will return `None` and raise a UserWarning.\n",
    "    This happens for patches that are either in the\n",
    "    cloud/shadow or seasonal snow set or if there exists no 19-label target.\n",
    "\n",
    "    Note: This works for Sentinel-2 and Sentinel-1 patch names!\n",
    "    \"\"\"\n",
    "    s1_train = get_s1_patches_from_original_train_split()\n",
    "    s2_train = get_s2_patches_from_original_train_split()\n",
    "    s1_validation = get_s1_patches_from_original_validation_split()\n",
    "    s2_validation = get_s2_patches_from_original_validation_split()\n",
    "    s1_test = get_s1_patches_from_original_test_split()\n",
    "    s2_test = get_s2_patches_from_original_test_split()\n",
    "\n",
    "    if patch in s1_train or patch in s2_train:\n",
    "        return ben_constants.Split.train\n",
    "    elif patch in s1_validation or patch in s2_validation:\n",
    "        return ben_constants.Split.validation\n",
    "    elif patch in s1_test or patch in s2_test:\n",
    "        return ben_constants.Split.test\n",
    "    warnings.warn(\n",
    "        \"Provided an input patch name which was not part of the original split.\",\n",
    "        UserWarning,\n",
    "    )\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20976/1935674112.py:27: UserWarning: Provided an input patch name which was not part of the original split.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "assert \"train\" == get_original_split_from_patch_name(\"S2A_MSIL2A_20170717T113321_28_87\")\n",
    "assert \"validation\" == get_original_split_from_patch_name(\n",
    "    \"S2B_MSIL2A_20170812T092029_75_6\"\n",
    ")\n",
    "assert \"test\" == get_original_split_from_patch_name(\"S2A_MSIL2A_20170717T113321_28_88\")\n",
    "\n",
    "assert get_original_split_from_patch_name(\"wrong_input\") is None\n",
    "\n",
    "# snowy patches\n",
    "assert get_original_split_from_patch_name(\"S2B_MSIL2A_20170906T101019_33_85\") is None\n",
    "\n",
    "assert \"train\" == get_original_split_from_patch_name(\n",
    "    \"S1A_IW_GRDH_1SDV_20170802T163350_34TCR_78_45\"\n",
    ")\n",
    "assert \"validation\" == get_original_split_from_patch_name(\n",
    "    \"S1B_IW_GRDH_1SDV_20170701T182622_29SND_64_30\"\n",
    ")\n",
    "assert \"test\" == get_original_split_from_patch_name(\"S2A_MSIL2A_20170717T113321_28_88\")\n",
    "assert (\n",
    "    get_original_split_from_patch_name(\"S1A_IW_GRDH_1SDV_20170904T161304_34VDN_33_85\")\n",
    "    is None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    get_original_split_from_patch_name(\"WrongName\")\n",
    "    assert len(w) == 1\n",
    "    assert issubclass(w[-1].category, UserWarning)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def _old2new_label(old_label: str) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Converts old-style BigEearthNet label to the\n",
    "    new labels.\n",
    "\n",
    "    > Note: Some labels were removed! This function\n",
    "    will return `None` if the label was removed and\n",
    "    raise a `KeyError` if the input label is unknown.\n",
    "    \"\"\"\n",
    "    return ben_constants.OLD2NEW_LABELS_DICT[old_label]\n",
    "\n",
    "\n",
    "def old2new_labels(old_labels: Iterable[str]) -> Optional[List[str]]:\n",
    "    \"\"\"\n",
    "    Converts a list of old-style BigEarthNet labels\n",
    "    to a list of labels.\n",
    "\n",
    "    If there are no corresponding new labels (which can happen with original BEN patches!)\n",
    "    then the function will return `None` and raise a user warning.\n",
    "\n",
    "    If an illegal/unknown input label is provided, a `KeyError` is raised.\n",
    "    \"\"\"\n",
    "    new_labels = [\n",
    "        _old2new_label(l) for l in old_labels if _old2new_label(l) is not None\n",
    "    ]\n",
    "    if len(old_labels) > 0 and len(new_labels) == 0:\n",
    "        warnings.warn(\n",
    "            \"Provided a list of old labels that only contains `removed` labels!\",\n",
    "            UserWarning,\n",
    "        )\n",
    "        new_labels = None\n",
    "    return new_labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Warning: Some of the original 43-class nomenclature patches have 0 labels with the 19-class nomenclature! This function might return `None` instead of an empty list!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20976/154519518.py:29: UserWarning: Provided a list of old labels that only contains `removed` labels!\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "with warnings.catch_warnings(record=True) as w:\n",
    "    warnings.simplefilter(\"always\")\n",
    "    old2new_labels([\"Burnt areas\"])\n",
    "    assert len(w) == 1\n",
    "    assert issubclass(w[-1].category, UserWarning)\n",
    "\n",
    "old2new_labels((\"Burnt areas\",))\n",
    "assert len(w) == 1\n",
    "\n",
    "with fc.ExceptionExpected(ex=KeyError):\n",
    "    old2new_labels([\"Illegal input label\"])\n",
    "\n",
    "fc.test_eq(\n",
    "    old2new_labels(\n",
    "        [\n",
    "            \"Continuous urban fabric\",\n",
    "            \"Discontinuous urban fabric\",\n",
    "        ]\n",
    "    ),\n",
    "    [\"Urban fabric\", \"Urban fabric\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "@validate_arguments\n",
    "def ben_19_labels_to_multi_hot(labels: Iterable[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Convenience function that converts an input iterable of labels into\n",
    "    a multi-hot encoded vector.\n",
    "    The naturally ordered label list is used as an encoder reference\n",
    "    - `bigearthnet_common.NEW_LABELS`\n",
    "\n",
    "    If an unknown label is given, a `KeyError` is raised.\n",
    "\n",
    "    Be aware that this approach assumes that **all** labels are actually used in the dataset!\n",
    "    This is not necessarily the case if you are using a subset!\n",
    "    For example, the \"Agro-forestry areas\" class is only present in Portugal and in no other country!\n",
    "    \"\"\"\n",
    "    idxs = [ben_constants.NEW_LABELS_TO_IDX[label] for label in labels]\n",
    "    multi_hot = fc.L([0] * len(ben_constants.NEW_LABELS))\n",
    "    multi_hot[idxs] = 1.0\n",
    "    return list(multi_hot)\n",
    "\n",
    "\n",
    "@validate_arguments\n",
    "def ben_43_labels_to_multi_hot(labels: Iterable[str]) -> List[float]:\n",
    "    \"\"\"\n",
    "    Convenience function that converts an input iterable of labels into\n",
    "    a multi-hot encoded vector.\n",
    "    The naturally ordered label list is used as an encoder reference\n",
    "    - `bigearthnet_common.OLD_LABELS`\n",
    "\n",
    "    If an unknown label is given, a `KeyError` is raised.\n",
    "\n",
    "    Be aware that this approach assumes that **all** labels are actually used in the dataset!\n",
    "    This is not necessarily the case if you are using a subset!\n",
    "    For example, the \"Agro-forestry areas\" class is only present in Portugal and in no other country!\n",
    "    \"\"\"\n",
    "    idxs = [ben_constants.OLD_LABELS_TO_IDX[label] for label in labels]\n",
    "    multi_hot = fc.L([0] * len(ben_constants.OLD_LABELS))\n",
    "    multi_hot[idxs] = 1.0\n",
    "    return list(multi_hot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agro = ben_19_labels_to_multi_hot((\"Agro-forestry areas\",))\n",
    "assert len(agro) == 19\n",
    "assert agro[0] == 1.0\n",
    "assert not any(agro[1:])\n",
    "\n",
    "multi = ben_19_labels_to_multi_hot((\"Agro-forestry areas\", \"Arable land\"))\n",
    "assert multi[0] == 1.0\n",
    "assert multi[1] == 1.0\n",
    "assert not any(multi[2:])\n",
    "\n",
    "with fc.ExceptionExpected(ex=KeyError):\n",
    "    ben_19_labels_to_multi_hot([\"Airports\"])\n",
    "\n",
    "agro43 = ben_43_labels_to_multi_hot((\"Agro-forestry areas\",))\n",
    "assert len(agro43) == 43\n",
    "assert agro43[0] == 1.0\n",
    "assert not any(agro43[1:])\n",
    "\n",
    "multi43 = ben_43_labels_to_multi_hot((\"Agro-forestry areas\", \"Airports\"))\n",
    "assert multi43[0] == 1.0\n",
    "assert multi43[1] == 1.0\n",
    "assert not any(multi43[2:])\n",
    "\n",
    "with fc.ExceptionExpected(ex=KeyError):\n",
    "    ben_43_labels_to_multi_hot([\"Arable land\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide\n",
    "# this should also work for 1-dimensional numpy arrays\n",
    "import numpy as np\n",
    "\n",
    "agro = ben_19_labels_to_multi_hot(np.array([\"Agro-forestry areas\"]))\n",
    "assert len(agro) == 19\n",
    "assert agro[0] == 1.0\n",
    "assert not any(agro[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def _are_s1_files_complete(patch_path: DirectoryPath) -> bool:\n",
    "    \"\"\"\n",
    "    Check if all S1-patch files exists (bands and json files) and are not empty.\n",
    "    \"\"\"\n",
    "    file_suffixes = [\"VV\", \"VH\", \"_labels_metadata.json\"]\n",
    "    for suffix in file_suffixes:\n",
    "        file = patch_path / f\"{patch_path.name}{suffix}\"\n",
    "        if not file.exists() or file.stat().st_size == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _are_s2_files_complete(patch_path: DirectoryPath) -> bool:\n",
    "    \"\"\"\n",
    "    Check if all S2-patch files exists (bands and json files) and are not empty.\n",
    "    \"\"\"\n",
    "    file_suffixes = [f\"_B{i:02}.tif\" for i in range(1, 13) if i != 10] + [\"_B8A\", \"_labels_metadata.json\"]\n",
    "    for suffix in file_suffixes:\n",
    "        file = patch_path / f\"{patch_path.name}{suffix}\"\n",
    "        if not file.exists() or file.stat().st_size == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def _print_missing_dirs(missing_dirs, show_num: int = 10) -> None:\n",
    "    rich.print(\"There are some missing directories!\")\n",
    "    rich.print(\"The following directories are missing compared to the complete BEN archive.\")\n",
    "    show_num = min(show_num, len(missing_dirs))\n",
    "    rich.print(f\"Showing only the first {show_num} invalid directories of {len(missing_dirs)}\")\n",
    "    rich.print([d for _, d in zip(range(show_num), missing_dirs)])\n",
    "\n",
    "def _print_dirs_with_missing_files(dirs_with_missing_files, show_num: int = 10) -> None:\n",
    "    rich.print(\"There are some invalid directories!\")\n",
    "    rich.print(\"The following directories are missing files.\")\n",
    "    show_num = min(show_num, len(dirs_with_missing_files))\n",
    "    rich.print(f\"Showing only the first {show_num} invalid directories of {len(dirs_with_missing_files)}\")\n",
    "    rich.print([d for _, d in zip(range(show_num), dirs_with_missing_files)])\n",
    "    \n",
    "\n",
    "@validate_arguments\n",
    "def _validate_ben_root_directory(dir_path: DirectoryPath, is_sentinel2: bool) -> Set[str]:\n",
    "    files = {f for f in dir_path.glob(\"*\")}\n",
    "    patch_names = get_all_s2_patch_names() if is_sentinel2 else get_all_s1_patch_names()\n",
    "    expected_directories = {dir_path / patch for patch in patch_names}\n",
    "    missing_directories = expected_directories - files\n",
    "    ben_dirs = expected_directories & files\n",
    "    completeness_checker = _are_s2_files_complete if is_sentinel2 else _are_s1_files_complete\n",
    "    directories_with_missing_files = {f for f in ben_dirs if not completeness_checker(f)}\n",
    "    if missing_directories == set() and directories_with_missing_files == set():\n",
    "        rich.print(\"Nothing seems to be missing.\")\n",
    "        rich.print(f\"The Sentinel directory {dir_path} looks complete.\")\n",
    "        return set()\n",
    "    if missing_directories != set():\n",
    "        _print_missing_dirs(missing_directories)\n",
    "    if directories_with_missing_files != set():\n",
    "        _print_dirs_with_missing_files(directories_with_missing_files)\n",
    "    return missing_directories | directories_with_missing_files\n",
    "\n",
    "def validate_ben_s2_root_directory(dir_path: Path) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Quickly check if all expected files from the BigEarthNet-S2 archive are present.\n",
    "\n",
    "    This funtion will _not_ check if the files are correct or if they were modified!\n",
    "    The function will perform a simple existence check and verify that each file is not empty.\n",
    "    Other files will be ignored.\n",
    "    \"\"\"\n",
    "    return _validate_ben_root_directory(dir_path, is_sentinel2=True)\n",
    "\n",
    "def validate_ben_s1_root_directory(dir_path: Path) -> Set[str]:\n",
    "    \"\"\"\n",
    "    Quickly check if all expected files from the BigEarthNet-S1 archive are present.\n",
    "\n",
    "    This funtion will _not_ check if the files are correct or if they were modified!\n",
    "    The function will perform a simple existence check and verify that each file is not empty.\n",
    "    Other files will be ignored.\n",
    "    \"\"\"\n",
    "    return _validate_ben_root_directory(dir_path, is_sentinel2=False)\n",
    "\n",
    "def validate_ben_s2_root_directory_cli():\n",
    "    app = typer.Typer()\n",
    "    app.command()(validate_ben_s2_root_directory)\n",
    "    app()\n",
    "\n",
    "def validate_ben_s1_root_directory_cli():\n",
    "    app = typer.Typer()\n",
    "    app.command()(validate_ben_s1_root_directory)\n",
    "    app()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are some missing directories!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are some missing directories!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The following directories are missing compared to the complete BEN archive.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The following directories are missing compared to the complete BEN archive.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Showing only the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> invalid directories of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">590324</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Showing only the first \u001b[1;36m10\u001b[0m invalid directories of \u001b[1;36m590324\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20180219T063851_29UPV_62_31'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1B_IW_GRDH_1SDV_20170709T043159_35VNL_71_7'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20180228T154921_34WFS_43_63'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20170617T064659_29UPU_32_17'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1B_IW_GRDH_1SDV_20171101T153951_35VNJ_68_84'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20180206T153231_35VPK_75_30'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1B_IW_GRDH_1SDV_20180130T162407_34TEP_22_78'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20170914T162451_34TEP_81_29'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20180417T155012_34WFV_20_45'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20180509T160338_35VLC_22_75'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20180219T063851_29UPV_62_31'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1B_IW_GRDH_1SDV_20170709T043159_35VNL_71_7'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20180228T154921_34WFS_43_63'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20170617T064659_29UPU_32_17'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1B_IW_GRDH_1SDV_20171101T153951_35VNJ_68_84'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20180206T153231_35VPK_75_30'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1B_IW_GRDH_1SDV_20180130T162407_34TEP_22_78'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20170914T162451_34TEP_81_29'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20180417T155012_34WFV_20_45'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20180509T160338_35VLC_22_75'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are some invalid directories!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are some invalid directories!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The following directories are missing files.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The following directories are missing files.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Showing only the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> invalid directories of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Showing only the first \u001b[1;36m2\u001b[0m invalid directories of \u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20170613T165043_33UUP_61_40'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S1_json_only/S1A_IW_GRDH_1SDV_20170613T165043_33UUP_61_39'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20170613T165043_33UUP_61_40'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S1_json_only/S1A_IW_GRDH_1SDV_20170613T165043_33UUP_61_39'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are some missing directories!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are some missing directories!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The following directories are missing compared to the complete BEN archive.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The following directories are missing compared to the complete BEN archive.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Showing only the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">10</span> invalid directories of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">590324</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Showing only the first \u001b[1;36m10\u001b[0m invalid directories of \u001b[1;36m590324\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20180421T100029_46_4'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20180525T094029_45_18'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20180515T112110_89_64'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2A_MSIL2A_20170613T101031_63_36'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20180224T112109_33_44'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20180417T102019_16_16'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20171112T114339_9_60'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20180204T94161_81_12'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2A_MSIL2A_20170905T095031_33_25'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2B_MSIL2A_20180515T112109_83_23'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20180421T100029_46_4'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20180525T094029_45_18'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20180515T112110_89_64'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2A_MSIL2A_20170613T101031_63_36'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20180224T112109_33_44'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20180417T102019_16_16'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20171112T114339_9_60'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20180204T94161_81_12'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2A_MSIL2A_20170905T095031_33_25'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2B_MSIL2A_20180515T112109_83_23'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">There are some invalid directories!\n",
       "</pre>\n"
      ],
      "text/plain": [
       "There are some invalid directories!\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">The following directories are missing files.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "The following directories are missing files.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Showing only the first <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span> invalid directories of <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Showing only the first \u001b[1;36m2\u001b[0m invalid directories of \u001b[1;36m2\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">[</span>\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2A_MSIL2A_20170617T113321_4_55'</span><span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Path</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008000; text-decoration-color: #008000\">'S2_json_only/S2A_MSIL2A_20170617T113321_36_85'</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m[\u001b[0m\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2A_MSIL2A_20170617T113321_4_55'\u001b[0m\u001b[1m)\u001b[0m,\n",
       "    \u001b[1;35mPath\u001b[0m\u001b[1m(\u001b[0m\u001b[32m'S2_json_only/S2A_MSIL2A_20170617T113321_36_85'\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# hide\n",
    "\n",
    "# The json path has no bands files and contains therefore no valid data\n",
    "invalid_path = Path(\"S1_json_only\")\n",
    "assert len(validate_ben_s1_root_directory(invalid_path)) == ben_constants.BEN_COMPLETE_SIZE\n",
    "\n",
    "invalid_path = Path(\"S2_json_only\")\n",
    "assert len(validate_ben_s2_root_directory(invalid_path)) == ben_constants.BEN_COMPLETE_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_constants.ipynb.\n",
      "Converted 01_base.ipynb.\n",
      "Converted 02_sets.ipynb.\n",
      "Converted index.ipynb.\n",
      "converting: /home/kai/git/bigearthnet_common/nbs/01_base.ipynb\n",
      "converting /home/kai/git/bigearthnet_common/nbs/index.ipynb to README.md\n"
     ]
    }
   ],
   "source": [
    "# hide\n",
    "from nbdev.cli import nbdev_build_docs\n",
    "from nbdev.export import notebook2script\n",
    "\n",
    "notebook2script()\n",
    "nbdev_build_docs()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
